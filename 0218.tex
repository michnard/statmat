%%%%lezione 18 febbraio%%%%

\subsection{Funzione generatrice dei momenti}
Lezione del 18/02, ultima modifica 04/03

\begin{definizione}
Sia $X$ una variabile casuale (discreta o assolutamente continua). Se esiste $t_0 > 0$ tale per cui $\mathbbm{E}(e^{tX} < +\infty)$ $\forall t \in (-t_0 , t_0)$, chiameremo la funzione 

$$M_X := \mathbbm{E}(e^{tX})$$

%togliere corsivo%
funzione generatrice dei Momenti di $X$.
\end{definizione}

\textbf{Esempi}
\begin{enumerate}
\item $X \sim b(1,p)$ con $p \in (0,1)$. Si ha:

$$M_X (t) 	= \mathbbm{E}(e^{tX}) 
			= \displaystyle\sum_{x=0}^1 e^{tx} \mathbbm{P}(X=x)
			= \displaystyle\sum_{x=0}^1 e^{tx} p^x (1-p)^{1-x}
			= p e^t + (1-p)$$	


\item $X \sim P(\lambda)$ con $\lambda > 0$. Si ha:

$$M_X (t) 	= \mathbbm{E}(e^{tX}) 
			= \displaystyle\sum_{x=0}^{+\infty} e^{tx} \frac{e^{-\lambda} \lambda^x}{x!}
			= e^{\lambda (e^t -1)}$$
			
			
\item $X \sim G(\alpha, \beta)$, ovvero 

$$f_X (x; \alpha, \beta) := \frac{1}{\Gamma (\alpha) \beta^{\alpha}} x^{\alpha -1} e^{-\frac{1}{\beta} x}$$ con $\alpha>0$, $\beta>0$, $x>0$ e 

$$\Gamma(\alpha) := \int_0^{+\infty} \! x^{\alpha -1} e^{-x} \mathrm{d}x$$ 

(nota: $\alpha \in \mathbbm{N} \Longrightarrow \Gamma(\alpha)=(\alpha - 1)!$)
\\
\\
Abbiamo che: 

$$M_X (t)	= \mathbbm{E}(e^{tX})
			= \int_0^{+\infty} \! e^{tx} \frac{1}{\Gamma (\alpha) \beta^{\alpha}} x^{\alpha -1} e^{-\frac{1}{\beta} x} \mathrm{d}x$$
\begin{center} $=$ ...[sostituzione $\sigma := x(\frac{1}{\beta} - t)$]...  \end{center}
			$$= \frac{1}{(1-\beta t)^{\alpha}}$$ con $t < \frac{1}{\beta}$
			
\end{enumerate}

\paragraph{Momenti di una variabile casuale}

\begin{definizione}
Se una variabile casuale ammette FGM derivabile quanto si vuole in un intorno di $t=0$ e se tutti i suoi momenti sono finiti, allora definiamo il momento di ordine $s$ non centrato: 

$$\mu'_s := \mathbbm{E} (X^s) = \frac{d^s}{dt^s} M_X (t) \! \mid_{t=0}$$ 

Il momento di di ordine $s$ centrato in $a \in \mathbbm{R}$ è: 

$$\mu_s (a) := \mathbbm{E} ((X-a)^s)$$

Ovvero $\mu'_s = \mu_s (0)$. Inoltre sappiamo che $\mu'_1 = \mathbbm{E} (X)$

Definiamo infine il momento di ordine $s$ centrato (in $\mu'_1$):

$$\mu_s := \mathbbm{E} ((X-\mu'_1)^s)$$

Osserviamo che $\mu_2 = \mathbbm{E} ((X-\mu'_1)^2) = Var(X) = \mathbbm{E} (X^2) - (\mathbbm{E} (X))^2 = \mu'_2 - (\mu'_1)^2$
\end{definizione}


***Io ho scritto sta roba...non capisco che cazzo è...***

$$\mathbbm{E} ((X-\mu'_1)^s) = \displaystyle\sum_{m=0}^s (-1)^m \binom{s}{m} \mu'_{s-m} (\mu'_1)^m$$

\begin{teo}
Date due (o più) v.c. $X$ e $Y$ aventi $f$ densità / $f$ massa $f_X$ e $f_Y$ e fgm $M_X(t)$ e $M_Y(t)$ rispettivamente e assunte $X$ e $Y$ essere indipendenti, allora si ha

$$M_{X+Y} = M_X(t) M_Y(t)$$
\end{teo}

\begin{teo}
Siano $X$ e $Y$ v.c. con funzioni di ripartizione $F_X(x)$ e $F_Y(y)$ rispettivamente. Siano $M_X(t)$ e $M_Y(t)$ le fgm di $X$ e $Y$. Se $M_X(t) = M_Y(t)$ per ogni $t$ in un intorno dell'origine, allora 

$$X \stackrel{d}{=} Y$$
\end{teo}

\begin{oss}
Il teorema appena visto ci dice sostanzialmente che, se esiste, la fgm caratterizza la distribuzione della corrispondente v.c.\\
\textbf{Esempio}
Siano $(X_1,...,X_n)$ risultati della replicazione di un esponenzionale casuiale dicotomico $(X_i \sim b(1,p))$. Vogliamo trovare la distribuzione di $S_n := \displaystyle\sum_{i=1}^n X_i$. Calcoliamo quindi la sua fgm:

$$M_{S_n}(t) = \mathbbm{E}({e^{tS_n}}) = \mathbbm{E}({e^{t \sum_{i=1}^n X_i}}) \stackrel{TEO 1}{=} \displaystyle\prod_{i=1}^n \mathbbm{E}({e^{tX_i}}) = \displaystyle\prod_{i=1}^n M_{X_i}(t) = \displaystyle\prod_{i=1}^n (p e^t + (1-p)) = (p e^t + (1-p))^n$$ ovvero $S_n$ è distribuita come $b(n,p)$ per il Teorema 2.
\end{oss}

\begin{esercizio}
Ripetere il calcolo precedente supponendo $X_i \sim P(\lambda), \forall i$.
\end{esercizio}


\subsection{Famiglia Esponenziale a $k$ parametri}
Una famiglia di $f$ densità / $f$ massa è detta essere una Famiglia Esponenziale a $k$ parametri $\theta_1,...,\theta_k$ se la corrispondente $f$ densità / $f$ massa (che è indicizzata da $\theta_1,...,\theta_k$) può essere scritta come

$$f_X(x;a) = C^*(x) D^*(\theta) \lbrace \displaystyle\sum_{m=1}^k A_m(\theta) B_m (x) \rbrace$$

dove $C^*(x)$ è una funzione della sola $x$, $D^*(\theta)$ è una funzione del solo $\theta$, $A_m(\theta)$ è una funzione del solo $\theta$ e $B_m(x)$ è una funzione della sola $x$.
\\
\\
\textbf{Esempi}
\begin{enumerate}
\item 
$X \sim G(\alpha, \beta) \Longrightarrow f_X(x; \alpha, \beta) = \frac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha -1} e^{- \frac{1}{\beta} x} \mathbbm{1}_{\mathbbm{R}^+}(x)$, $\alpha >0$, $\beta >0$
$\mathbbm{1}_{\mathbbm{R}^+}$ è detto supporto della distribuzione.
Quindi possiamo riscrivere $f_X(x; \alpha, \beta)$ come
$$f_X(x; \alpha, \beta) = \frac{1}{\Gamma(\alpha) \beta^\alpha} \mathbbm{1}_{\mathbbm{R}^+}(x) exp((\alpha -1) ln(x) - \frac{1}{\beta} x)$$
e quindi ponendo $D^*(\alpha, \beta) := \frac{1}{\Gamma(\alpha) \beta^\alpha}$, $C^*(x) := \mathbbm{1}_{\mathbbm{R}^+}(x)$, $A_1(\alpha, \beta) := (\alpha -1)$, $B_1(x) := ln(x)$, $A_2(\alpha, \beta) := - \frac{1}{\beta}$ e $B_2(x) := x$, otteniamo $G(\alpha, \beta)$ come famiglia esponenziale con $k=2$.

\item $X \sim b(n,p) \Longrightarrow f_X(x; n,p) = \binom{n}{x} p^x (1-p)^{n-x} \mathbbm{1}_{\lbrace 0,1,...,n \rbrace}(x)$ con $n \in \mathbbm{N}$ noto.
Quindi possiamo riscrivere $f_X(x; n,p)$ come
$$f_X(x; n,p) = \binom{n}{x} \mathbbm{1}_{\lbrace 0,1,...,n \rbrace}(x) (1-p)^n exp(ln(\frac{p}{1-p}) x)$$ con $\frac{p}{1-p}$ detto odd ratio o parametra naturale della famiglia esponenziale.\\
Quindi ponendo $D^*(p) := (1-p)^n$, $C^*(x) := \binom{n}{x} \mathbbm{1}_{\lbrace 0,1,...,n \rbrace}(x)$, $A_1(p) := ln(\frac{p}{1-p})$, $B_1(x) := x$, otteniamo $b(n,p)$ come famiglia esponenziale con $k=1$.
\end{enumerate}

\begin{oss}
Le famiglie di esponenziali di ?...? hanno interessanti proprietà matematiche (proprietà di regolarità).\\
Dal punto di vista statistico, ciò si traduce in un'interessante conseguenza: tutta l'informazione contenuta nei dati a disposizione $(X_1,...,X_n)$ relativa alla funzione $f_X (x; \theta)$ può essere sintetizzata attraverso $k$ quantità (funzioni di $(X_1,...,X_n)$) che potranno essere impiegate per costruire procedure inferenziali (stima, test per la verifica di ipotesi) riguardanti il parametro $\theta$.\\
Ovvero, l'appartenenza a una famiglia esponenziale permette una riduzione dei dati $(X_1,...,X_n)$ via $B_m$.\end{oss}